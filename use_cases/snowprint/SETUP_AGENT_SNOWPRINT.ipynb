{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qisija3lyfadftjmqcn7",
   "authorId": "2022491430521",
   "authorName": "ADMIN",
   "authorEmail": "michael.gorkow@snowflake.com",
   "sessionId": "9ae0e8af-9431-46e7-a803-6d07a9c36309",
   "lastEditTime": 1741759845980
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66bee02-de2d-4c96-92dc-2a683e8be305",
   "metadata": {
    "collapsed": false,
    "name": "OVERVIEW"
   },
   "source": [
    "# Cortex Agents\n",
    "In this notebook you will setup multiple Cortex Search and Cortex Analyst Services which will be used by Cortex Agents to answer user queries on unstructured and structured data.\n",
    "![text](https://github.com/michaelgorkow/snowflake_cortex_agents_demo/blob/main/resources/cortex_agents_notebook_small.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2c5bc2ff-479d-4ff4-94a6-453a33291a18",
   "metadata": {
    "language": "python",
    "name": "IMPORTS"
   },
   "outputs": [],
   "source": "import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport random\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport streamlit as st\nfrom faker import Faker\n\nfrom snowflake.core import Root\nfrom snowflake.cortex import Complete\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import col\n\n# Set seed for reproducibility\nrandom.seed(42)\n\nsession = get_active_session()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2cda5f3-3dc6-4609-bf2d-12d114adb9af",
   "metadata": {
    "collapsed": false,
    "name": "CORTEX_SEARCH1"
   },
   "source": [
    "# Setup the Cortex Search Service [Unstructured Data]\n",
    "\n",
    "We have some PDF documents in our stage **DOCUMENTS** that we want business users to be able to ask questions about.  \n",
    "To achieve this, we need to extract the contents of the PDF files and make them searchable.\n",
    "\n",
    "## Extracting Content from PDF Files\n",
    "\n",
    "### [`PARSE_DOCUMENT`](https://docs.snowflake.com/en/sql-reference/functions/parse_document-snowflake-cortex)  \n",
    "This function returns the extracted content from a document on a Snowflake stage as an **OBJECT** that contains JSON-encoded objects as strings.  \n",
    "\n",
    "It supports two types of extractions:  \n",
    "- **Optical Character Recognition (OCR)**  \n",
    "- **Layout Extraction**  \n",
    "\n",
    "### [`SPLIT_TEXT_RECURSIVE_CHARACTER`](https://docs.snowflake.com/en/sql-reference/functions/split_text_recursive_character-snowflake-cortex)  \n",
    "The `SPLIT_TEXT_RECURSIVE_CHARACTER` function splits a string into shorter strings recursively. It is useful for preprocessing text to be used with text embedding or search indexing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cb35b-78ee-4091-97c2-0b05443d8e52",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_SEARCH2"
   },
   "outputs": [],
   "source": "-- List documents in stage\nSELECT * FROM DIRECTORY('@DOCUMENTS');"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0d2d82-16f6-4552-bc50-d56f91ce30d3",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_SEARCH3"
   },
   "outputs": [],
   "source": "-- Layout extraction for PDF documents\nCREATE TABLE IF NOT EXISTS RAW_TEXT AS\nSELECT \n    RELATIVE_PATH,\n    TO_VARCHAR (\n        SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n            '@DOCUMENTS',\n            RELATIVE_PATH,\n            {'mode': 'LAYOUT'} ):content\n        ) AS EXTRACTED_LAYOUT,\n    -- A custom Python UDF that was created during the demo setup\n    remove_duplicate_headers(EXTRACTED_LAYOUT) AS CLEANED_LAYOUT\nFROM \n    DIRECTORY('@DOCUMENTS');\n\nSELECT * FROM RAW_TEXT;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ec3abd-9a67-4d63-a7cb-d22196990390",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_SEARCH4"
   },
   "outputs": [],
   "source": "-- Create chunks from extracted content\nCREATE TABLE IF NOT EXISTS CHUNKED_TEXT AS\nSELECT\n   RELATIVE_PATH,\n   c.INDEX::INTEGER AS CHUNK_INDEX,\n   c.value::TEXT AS CHUNK_TEXT\nFROM\n   RAW_TEXT,\n   LATERAL FLATTEN( input => SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER (\n      CLEANED_LAYOUT,\n      'markdown',\n      1800,\n      0,\n      ['\\n\\n', '\\n', ' ', '']\n   )) c;\n\nSELECT * FROM CHUNKED_TEXT;"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c651cd6-a275-4a07-9b85-52a1554937fa",
   "metadata": {
    "language": "sql",
    "name": "CORTEX_SEARCH5"
   },
   "outputs": [],
   "source": "-- Create a Cortex Search Service for Annual Reports\nCREATE CORTEX SEARCH SERVICE IF NOT EXISTS SNOWPRINT_PRODUCT_GUIDES\n  ON CHUNK_TEXT\n  ATTRIBUTES RELATIVE_PATH, CHUNK_INDEX\n  WAREHOUSE = COMPUTE_WH\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      CHUNK_TEXT,\n      RELATIVE_PATH,\n      CHUNK_INDEX\n  FROM CHUNKED_TEXT\n);"
  },
  {
   "cell_type": "markdown",
   "id": "32f8ca07-db9b-4cf5-b2a5-e5a3e030aaf0",
   "metadata": {
    "collapsed": false,
    "name": "CORTEX_SEARCH7"
   },
   "source": "### (Optional) Test Your Service in a Simple RAG Pipeline  \n\nIn this small example, we **combine Cortex Search with Cortex LLMs** to generate a response from contextâ€”also known as **Retrieval-Augmented Generation (RAG)**.  \nThis approach enhances responses by retrieving relevant data before generating an answer, improving accuracy and contextual relevance. ðŸš€  "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e07ffc-0874-422b-a439-dacdaab98585",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "CORTEX_SEARCH8"
   },
   "outputs": [],
   "source": "root = Root(get_active_session())\n\nquestion = 'How do I set up and configure a new user in the Prinect system?'\n\n# Fetch service\nmy_service = (root\n  .databases[\"CORTEX_AGENTS_DEMO\"]\n  .schemas[\"SNOWPRINT\"]\n  .cortex_search_services[\"SNOWPRINT_PRODUCT_GUIDES\"]\n)\n\n# Query service\nresp = my_service.search(\n  query=question,\n  columns=[\"CHUNK_INDEX\", \"CHUNK_TEXT\", \"RELATIVE_PATH\"],\n  limit=1\n)\nresp = resp.results[0]\n\nst.info(f'# File: {resp[\"RELATIVE_PATH\"]}\\n\\n {resp[\"CHUNK_TEXT\"]}')\n\n# Generate Response\nmodel = 'mistral-large2'\nprompt = f\"{question} Answer based on the provided context: {resp['CHUNK_TEXT']}\"\nresponse = Complete(model, prompt).strip()\n\nst.info(f'# LLM Response: \\n\\n {response}')"
  },
  {
   "cell_type": "markdown",
   "id": "00717c52-d652-48c9-be0b-1aaf5d0e2748",
   "metadata": {
    "collapsed": false,
    "name": "CORTEX_ANALYST1"
   },
   "source": "# Generate Data for Cortex Analst [Structured Data]  \n\n## Dataset\nWe are generating a **fictional dataset** from _Snowprint AG_. The dataset is split into three core tables that capture different aspects of the companyâ€™s printing operations:\n\n1. **Customers**  \n   Stores client information such as name, contact details, location, and phone numbers.  \n   Example use: Identifying customer segments by country or city, generating targeted marketing campaigns, or pulling up client contact details on demand.\n\n2. **Jobs**  \n   Tracks printing jobs with columns for job name, priority, due date, and completion status.  \n   Example use: Monitoring overall job status, identifying high-priority tasks, or finding overdue projects.\n\n3. **Steps**  \n   Details the step-by-step process for each job (e.g., _Prepare_, _Printing_, _Quality Checks_) along with timestamps.  \n   Example use: Analyzing production workflows, calculating turnaround times for each step, or optimizing bottlenecks.\n\n## Business Use Case Examples\nUsers will be able to get insights into following topics:\n- **Customer Relationship Management**: Quickly view customers by region or generate lists of pending orders.\n- **Production Tracking**: Identify high-priority jobs, check which steps are completed, or spot delays in real time.\n- **Performance Analytics**: Calculate average lead times, track overall productivity, and improve scheduling and resource allocation.\n"
  },
  {
   "cell_type": "code",
   "id": "81de114f-1928-4cfa-9af4-fbf642380036",
   "metadata": {
    "language": "python",
    "name": "CORTEX_ANALYST2"
   },
   "outputs": [],
   "source": "# List of European countries and their corresponding phone codes\ncountries = [\"Germany\", \"France\", \"Italy\", \"Spain\", \"United Kingdom\", \"Netherlands\", \"Belgium\", \"Switzerland\", \"Austria\", \"Sweden\"]\ncountry_codes = {\n    \"Germany\": \"+49\",\n    \"France\": \"+33\",\n    \"Italy\": \"+39\",\n    \"Spain\": \"+34\",\n    \"United Kingdom\": \"+44\",\n    \"Netherlands\": \"+31\",\n    \"Belgium\": \"+32\",\n    \"Switzerland\": \"+41\",\n    \"Austria\": \"+43\",\n    \"Sweden\": \"+46\"\n}\n\n# Mapping of countries to locale-specific Faker instances for realistic addresses\nfaker_locales = {\n    \"Germany\": Faker(\"de_DE\"),\n    \"France\": Faker(\"fr_FR\"),\n    \"Italy\": Faker(\"it_IT\"),\n    \"Spain\": Faker(\"es_ES\"),\n    \"United Kingdom\": Faker(\"en_GB\"),\n    \"Netherlands\": Faker(\"nl_NL\"),\n    \"Belgium\": Faker(\"fr_BE\"),\n    \"Switzerland\": Faker(\"de_CH\"),\n    \"Austria\": Faker(\"de_AT\"),\n    \"Sweden\": Faker(\"sv_SE\")\n}\n\n# Create a default Faker instance for company and person names\nfake = Faker()\n\ndef generate_company_name():\n    return fake.company()\n\ndef generate_contact_person():\n    return fake.name()\n\ndef generate_email(contact_person, company_name):\n    # Use the contact person's first and last names and a domain based on the company name.\n    first, last = contact_person.split(\" \", 1)\n    # Remove spaces and punctuation from the company name and convert to lowercase.\n    domain = \"\".join([c for c in company_name if c.isalnum()]).lower() + \".com\"\n    return f\"{first.lower()}.{last.lower()}@{domain}\"\n\ndef generate_phone_number(country):\n    code = country_codes[country]\n    # Generate a realistic phone number: country code, then three groups of digits.\n    area = random.randint(100, 999)\n    part1 = random.randint(100, 999)\n    part2 = random.randint(1000, 9999)\n    return f\"{code} {area} {part1} {part2}\"\n\n# Generate sample data for 100 Customers\ncustomer_data = []\ncustomer_ids = [f'C_{i:06d}' for i in range(1,100)]\nfor cid in customer_ids:\n    customer_id = cid\n    company_name = generate_company_name()\n    contact_person = generate_contact_person()\n    country = random.choice(countries)\n    email = generate_email(contact_person, company_name)\n    phone = generate_phone_number(country)\n    \n    # Use the locale-specific Faker instance to generate city and street address\n    locale_fake = faker_locales[country]\n    city = locale_fake.city()\n    street_address = locale_fake.street_address().replace('\\n','')\n    \n    customer_data.append({\n        \"CUSTOMER_ID\": customer_id,\n        \"CUSTOMER_NAME\": company_name,\n        \"CONTACT_PERSON\": contact_person,\n        \"EMAIL\": email,\n        \"COUNTRY\": country,\n        \"CITY\": city,\n        \"STREET_ADDRESS\": street_address,\n        \"PHONE_NUMBER\": phone\n    })\n\ncustomers = pd.DataFrame(customer_data)\n\njobs_list = []\njob_processing_steps_list = []\njob_id_counter = 1\n\n# Fixed processing sequence\nprocessing_sequence = [\"Qualify\", \"Prepare\", \"Imposition\", \"Proof\", \"Print\"]\n\n# For each customer, generate between 100 and 200 jobs.\nfor cust_id in customer_ids:\n    num_jobs = random.randint(100, 200)\n    for i in range(num_jobs):\n        # Generate job creation date within the last 30 days\n        created_at_dt = datetime.now() - timedelta(days=random.randint(1, 30),\n                                                     hours=random.randint(0, 23),\n                                                     minutes=random.randint(0, 59))\n        # Due date: after the job is created (between 1 and 30 days later)\n        due_date_dt = created_at_dt + timedelta(days=random.randint(1, 30))\n        \n        # Decide final job status with weights:\n        # Letâ€™s assume: Pending (20%), In Progress (20%), Completed (50%), Canceled (10%)\n        final_status = random.choices([\"Pending\", \"In Progress\", \"Completed\", \"Canceled\"],\n                                      weights=[20, 20, 50, 10])[0]\n        \n        job_id = f'JOB_{job_id_counter:06d}'\n        job_id_counter += 1\n        \n        jobs_list.append({\n            \"JOB_ID\": job_id,\n            \"JOB_NAME\": f\"Print Job {job_id_counter}\",\n            \"CUSTOMER_ID\": cust_id,\n            \"STATUS\": final_status,\n            \"DUE_DATE\": due_date_dt.strftime('%Y-%m-%d'),\n            \"CREATED_AT\": created_at_dt.strftime('%Y-%m-%d %H:%M:%S'),\n            \"JOB_PRIORITY\": random.choice([\"High\", \"Medium\", \"Low\"]),\n        })\n        \n        # Create processing steps only if the job is no longer Pending.\n        if final_status != \"Pending\":\n            # For a \"Completed\" job, we always use all 5 steps.\n            # For jobs \"In Progress\" or \"Canceled\", randomly choose 1 to 4 steps.\n            if final_status == \"Completed\":\n                steps_count = 5\n            else:\n                steps_count = random.randint(1, 4)\n                \n            # Base time for processing steps starts after the job's creation.\n            step_time = created_at_dt + timedelta(hours=random.randint(1, 12))\n            \n            # Create steps in the fixed order\n            for step_index in range(steps_count):\n                step_name = processing_sequence[step_index]\n                # Reset STEP_ID for each job so that it starts with STEP_1\n                step_id = f'STEP_{step_index+1}'\n                \n                is_last = (step_index == steps_count - 1)\n                if is_last:\n                    if final_status == \"Completed\":\n                        step_status = \"Completed\"\n                    elif final_status == \"In Progress\":\n                        step_status = \"In Progress\"\n                    elif final_status == \"Canceled\":\n                        step_status = \"Canceled\"\n                else:\n                    step_status = \"Completed\"\n                \n                # The step's start time is the current step_time.\n                started_at = step_time\n                \n                # If the step is completed, add a duration (30 to 120 minutes)\n                if step_status == \"Completed\":\n                    duration = timedelta(minutes=random.randint(30, 120))\n                    completed_at = started_at + duration\n                    # Next step starts a little after the previous stepâ€™s completion\n                    step_time = completed_at + timedelta(minutes=random.randint(10, 30))\n                else:\n                    completed_at = None  # in-progress or canceled step is not finished\n                \n                job_processing_steps_list.append({\n                    \"STEP_ID\": step_id,\n                    \"JOB_ID\": job_id,\n                    \"SEQUENCE_NAME\": step_name,\n                    \"STARTED_AT\": started_at.strftime('%Y-%m-%d %H:%M:%S'),\n                    \"COMPLETED_AT\": completed_at.strftime('%Y-%m-%d %H:%M:%S') if completed_at else None,\n                    \"STATUS\": step_status,\n                })\n\n# Create DataFrames\njobs = pd.DataFrame(jobs_list)\njob_processing_steps = pd.DataFrame(job_processing_steps_list)\n\ncustomers = session.create_dataframe(customers, schema=customers.columns)\ncustomers.write.save_as_table(table_name='CUSTOMERS', mode='overwrite')\nsession.table('CUSTOMERS').show()\n\njobs = session.create_dataframe(jobs, schema=jobs.columns)\njobs = jobs.with_column(\"DUE_DATE\", col(\"DUE_DATE\").cast(T.DateType()))\njobs = jobs.with_column(\"CREATED_AT\", col(\"CREATED_AT\").cast(T.DateType()))\njobs.write.save_as_table(table_name='JOBS', mode='overwrite')\nsession.table('JOBS').show()\n\njob_processing_steps = session.create_dataframe(job_processing_steps, schema=job_processing_steps.columns)\njob_processing_steps = job_processing_steps.with_column(\"STARTED_AT\", col(\"STARTED_AT\").cast(T.TimestampType()))\njob_processing_steps = job_processing_steps.with_column(\"COMPLETED_AT\", col(\"COMPLETED_AT\").cast(T.TimestampType()))\njob_processing_steps.write.save_as_table(table_name='JOB_PROCESS_STEPS', mode='overwrite')\nsession.table('JOB_PROCESS_STEPS').show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "352d9b4d-9c34-4e91-9cc6-caa60b231b2f",
   "metadata": {
    "collapsed": false,
    "name": "CORTEX_ANALYST3"
   },
   "source": "# Dynamic Literal Retrieval with Cortex Analyst\n\nBusiness users may not have detailed knowledge of how data is stored in Snowflake.  \nInstead of ingesting all possible values of a column into **Cortex Analyst**, we will use **dynamic literal retrieval** via the [Cortex Search Integration](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst/cortex-analyst-search-integration).\n\n## How It Works  \nWhen a user asks a question about that requires any of the columns we define in the Cortex Search Services, **Cortex Analyst** will:  \n1. Retrieve the relevant literal dynamically from the respective **Cortex Search Service**  \n2. Use the retrieved literal for **SQL generation**  \n\nThis approach ensures efficient and accurate query generation for columns with high cardinality without preloading all possible values into Cortex Analyst.  \nAnother major benefit is that the Cortex Search Service is updated automatically so that new values will be available to Cortex Analyst immediately without any operational work.\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837aa585-3963-4767-9f6e-df919c4faf6a",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_ANALYST4"
   },
   "outputs": [],
   "source": "CREATE CORTEX SEARCH SERVICE IF NOT EXISTS _ANALYST_SEARCH_CUSTOMER_NAME\n  ON CUSTOMER_NAME\n  WAREHOUSE = COMPUTE_WH\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      DISTINCT CUSTOMER_NAME\n  FROM CUSTOMERS\n);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cec9e-4070-4fcc-af17-11747b58b0c7",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_ANALYST5"
   },
   "outputs": [],
   "source": "CREATE CORTEX SEARCH SERVICE IF NOT EXISTS _ANALYST_SEARCH_CONTACT_PERSON\n  ON CONTACT_PERSON\n  WAREHOUSE = COMPUTE_WH\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      DISTINCT CONTACT_PERSON,\n  FROM CUSTOMERS\n);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822ae0c-101f-4f36-9252-b7e13134b3f1",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CORTEX_ANALYST6"
   },
   "outputs": [],
   "source": "CREATE CORTEX SEARCH SERVICE IF NOT EXISTS _ANALYST_SEARCH_COUNTRY\n  ON COUNTRY\n  WAREHOUSE = COMPUTE_WH\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      DISTINCT COUNTRY,\n  FROM CUSTOMERS\n);"
  },
  {
   "cell_type": "code",
   "id": "8aeca026-ae3f-4d1e-a177-27806d748579",
   "metadata": {
    "language": "sql",
    "name": "CORTEX_ANALYST7"
   },
   "outputs": [],
   "source": "CREATE CORTEX SEARCH SERVICE IF NOT EXISTS _ANALYST_SEARCH_CITY\n  ON CITY\n  WAREHOUSE = COMPUTE_WH\n  TARGET_LAG = '1 hour'\n  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\nAS (\n  SELECT\n      DISTINCT CITY,\n  FROM CUSTOMERS\n);",
   "execution_count": null
  }
 ]
}